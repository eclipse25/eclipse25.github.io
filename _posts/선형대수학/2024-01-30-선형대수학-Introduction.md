---
layout: post
title: "선형대수학 - Introduction"
categories: [선형대수학]
tags: [선형대수학, 머신러닝]
---

# 머신러닝을 위한 선형대수학

## **왜 선형대수학을 알아야 할까?**

행렬이라고 하면 고등학생 시절부터 익히 들어왔지만, 교육과정에서 빠지면서 배우지 못했고 대학에 입학한 후 개념도 모르는 상태에서 수업에서 쓰이는 바람에 인터넷을 뒤져가며 혼자 공부를 해본 적은 있지만 체계적으로 배운적이 없어 늘 내 마음 속 한구석에 불편하게 남아있는 존재였다. 그렇지만 선형대수와 행렬의 개념은 전자공학에서도 컴퓨터공학에서도 꼭 필요한데, 특히 머신러닝에 대해 공부를 하게 되었다면 한번쯤은 제대로 정리할 필요가 있다.<br>
그렇다면.. 왜 머신러닝을 공부할 때 선형대수학을 알아야만 하는 것인가..?

### **1. 데이터 표현 및 처리**

- **벡터와 행렬**은 머신러닝과 딥러닝에서 데이터를 표현하고 처리하는 기본 단위이다.
- 선형대수학은 이러한 **데이터 구조를 이해하고 효율적으로 조작하는 방법을 제공**한다.
  ```python
  # 예시: 이미지 데이터는 행렬로 표현되어 각 픽셀의 강도를 나타낸다.
  image_matrix = [[255, 0, 0],
                  [0, 255, 0],
                  [0, 0, 255]]
  ```

### **2. 알고리즘 이해 및 구현**

- 많은 머신러닝 알고리즘들이 선형대수학의 개념을 기반으로 한다.
- 예를 들어, **선형 회귀, SVM, PCA** 등의 알고리즘은 선형대수학적 연산에 의존한다.
  ```python
  # 예시: 선형 회귀에서 가중치와 데이터의 곱셈으로 예측값 계산
  weights = [0.5, 0.3]  # 가중치
  data_point = [1, 2]   # 데이터 포인트
  prediction = sum(w * x for w, x in zip(weights, data_point))
  ```

### **3. 최적화 및 연산 효율성**

- 머신러닝 모델의 학습은 **최적화 문제**로 볼 수 있다.
- 선형대수학은 **행렬 연산을 통해** 이러한 최적화 과정을 **효율적으로 수행하는 데 도움을 준다**.
  ```python
  # 예시: 그래디언트 하강법을 사용한 가중치 업데이트
  weight_gradient = -0.1  # 가중치에 대한 그래디언트
  learning_rate = 0.01    # 학습률
  weight = weight - learning_rate * weight_gradient
  ```

### **4. 딥러닝 모델의 근간**

- 딥러닝의 핵심 구성 요소인 **신경망**은 선형대수학적 개념을 광범위하게 사용한다.
- 가중치, 활성화 함수, 손실 함수 등 신경망의 각 요소는 **선형대수학으로 설명되고 계산된다**.
  ```python
  # 예시: 신경망의 행렬 곱셈
  weights = [[0.2, 0.8], [0.6, 0.4]]
  input_vector = [1, 2]
  output = [sum(w * x for w, x in zip(weights_row, input_vector)) for weights_row in weights]
  ```

### **5. 고급 주제에 대한 이해**

- 선형대수학은 머신러닝과 딥러닝의 **고급 주제들을 이해하는 데 필수적**이다.
- **특이값 분해(SVD), 행렬 분해, 고유값 문제** 등 고급 개념들의 이해에 선형대수학이 중요하다.

  ```python
  # 예시: 특이값 분해(SVD)를 사용한 데이터 압축
  import numpy as np
  matrix = np.array([[1, 2], [3, 4], [5, 6]])
  U, s, Vt = np.linalg.svd(matrix)
  ```

정리하면, 벡터와 행렬을 사용하는 것은 머신러닝과 딥러닝에서 데이터를 표현하고 처리하는데 있어서 중요한 역할을 하고 있기 때문에 선형대수학 공부는 머신러닝 공부를 시작하는데 있어 필수적이다. 그런 이유에서 기초적인 선형대수학을 정리해보려고 한다!
